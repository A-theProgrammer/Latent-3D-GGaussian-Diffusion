{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZzAOrhXE13J"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install open3d\n",
        "!pip install gradio\n",
        "# or\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "1BSpiyO0FERs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L3DG: Latent 3D Gaussian Diffusion -  Implementation and Demo\n",
        "---\n",
        "### Theoretical Background:\n",
        "\n",
        "This code implements a simplified version of the L3DG approach for generative 3D modeling. It involves two key stages:\n",
        "\n",
        "#### 1. Latent Compression via VQ-VAE:\n",
        "- A VQ-VAE is used to compress complex 3D point cloud data\n",
        "into a lower-dimensional latent space.\n",
        "- Vector quantization discretizes the latent representation,\n",
        "reducing complexity while preserving structural information.\n",
        "\n",
        "#### 2. Latent Diffusion Modeling:\n",
        "- A diffusion model operates in the learned latent space.\n",
        "- It gradually denoises latent vectors, enabling generation\n",
        "of new, realistic 3D structures from random noise."
      ],
      "metadata": {
        "id": "ElpAL46oGG2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Synthetic Data Generation\n",
        "# -----------------------------\n",
        "class Synthetic3DDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, num_points=1024):\n",
        "        # Each sample is a point cloud of shape (num_points, 3)\n",
        "        self.data = [np.random.rand(num_points, 3).astype(np.float32) for _ in range(num_samples)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Normalize each point cloud to [0, 1]\n",
        "        return torch.tensor(self.data[idx])\n",
        "\n",
        "# -----------------------------\n",
        "# VQ-VAE Components\n",
        "# -----------------------------\n",
        "# Encoder: maps 3D point cloud to a latent vector\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=128, latent_dim=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, N, 3]\n",
        "        B, N, _ = x.shape\n",
        "        x = x.view(B * N, -1)\n",
        "        z = self.net(x)\n",
        "        z = z.view(B, N, -1)  # reshape back to (B, N, latent_dim)\n",
        "        # For simplicity, we aggregate over points via mean\n",
        "        z = z.mean(dim=1)  # [B, latent_dim]\n",
        "        return z\n",
        "\n",
        "# Vector Quantizer: converts continuous latent vectors into discrete codebook entries\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings=512, embedding_dim=64, commitment_cost=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: [B, latent_dim]\n",
        "        # Compute distances between z and embedding vectors\n",
        "        # Expand z: [B, 1, latent_dim] and embeddings: [1, num_embeddings, latent_dim]\n",
        "        z_expanded = z.unsqueeze(1)\n",
        "        embeddings = self.embedding.weight.unsqueeze(0)\n",
        "        distances = torch.sum((z_expanded - embeddings)**2, dim=2)  # [B, num_embeddings]\n",
        "\n",
        "        # Get the nearest embedding indices\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        z_q = self.embedding(encoding_indices)\n",
        "\n",
        "        # Compute the loss for the VQ-VAE\n",
        "        loss = torch.mean((z_q.detach() - z)**2) + self.commitment_cost * torch.mean((z_q - z.detach())**2)\n",
        "        # Straight-through estimator\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        return z_q, loss\n",
        "\n",
        "# Decoder: reconstructs the point cloud from the quantized latent code\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=128, output_dim=3, num_points=1024):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_points = num_points\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_points * output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: [B, latent_dim]\n",
        "        x_recon = self.net(z)\n",
        "        x_recon = x_recon.view(z.size(0), self.num_points, -1)  # [B, num_points, 3]\n",
        "        return x_recon\n",
        "\n",
        "# Full VQ-VAE Model\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=128, latent_dim=64, num_embeddings=512, num_points=1024):\n",
        "        super(VQVAE, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.vq = VectorQuantizer(num_embeddings, latent_dim)\n",
        "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim, num_points)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z_q, vq_loss = self.vq(z)\n",
        "        x_recon = self.decoder(z_q)\n",
        "        return x_recon, vq_loss\n",
        "\n",
        "# -----------------------------\n",
        "# Training the VQ-VAE\n",
        "# -----------------------------\n",
        "def train_vqvae(model, dataloader, epochs=10, lr=1e-3, device='cpu'):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)  # [B, num_points, 3]\n",
        "            optimizer.zero_grad()\n",
        "            recon, vq_loss = model(batch)\n",
        "            # Reconstruction loss: L2 loss\n",
        "            recon_loss = ((recon - batch) ** 2).mean()\n",
        "            loss = recon_loss + vq_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Running the VQ-VAE Training\n",
        "# -----------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Prepare data\n",
        "    dataset = Synthetic3DDataset(num_samples=1000, num_points=1024)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Instantiate model\n",
        "    vqvae_model = VQVAE(input_dim=3, hidden_dim=128, latent_dim=64, num_embeddings=512, num_points=1024)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Train VQ-VAE\n",
        "    train_vqvae(vqvae_model, dataloader, epochs=20, lr=1e-3, device=device)\n",
        "\n",
        "    # Save the trained VQ-VAE for later use\n",
        "    torch.save(vqvae_model.state_dict(), \"vqvae_model.pth\")"
      ],
      "metadata": {
        "id": "yrctdZgeFITQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Diffusion Model Components\n",
        "# -----------------------------\n",
        "class LatentDiffusion(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=128):\n",
        "        super(LatentDiffusion, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, z_noisy):\n",
        "        # Predict the \"denoised\" latent code\n",
        "        return self.net(z_noisy)\n",
        "\n",
        "# -----------------------------\n",
        "# Diffusion Training Utilities\n",
        "# -----------------------------\n",
        "def add_noise(z, noise_level):\n",
        "    noise = torch.randn_like(z) * noise_level\n",
        "    return z + noise\n",
        "\n",
        "def train_diffusion(model, vqvae, dataloader, epochs=10, lr=1e-3, device='cpu'):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "    vqvae.to(device)\n",
        "    vqvae.eval()  # Freeze the VQ-VAE during diffusion training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            # Encode the clean latent codes with VQ-VAE encoder (we do not quantize here)\n",
        "            with torch.no_grad():\n",
        "                z_clean = vqvae.encoder(batch)\n",
        "            # Add noise to the latent codes\n",
        "            noise_level = 0.1  # You can adjust this schedule\n",
        "            z_noisy = add_noise(z_clean, noise_level)\n",
        "            # Model predicts the clean latent code from the noisy version\n",
        "            z_pred = model(z_noisy)\n",
        "            loss = ((z_pred - z_clean)**2).mean()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Diffusion Epoch {epoch+1}/{epochs} Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Running the Diffusion Model Training\n",
        "# -----------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Reload VQ-VAE model (if not already in memory)\n",
        "    vqvae_model = VQVAE(input_dim=3, hidden_dim=128, latent_dim=64, num_embeddings=512, num_points=1024)\n",
        "    vqvae_model.load_state_dict(torch.load(\"vqvae_model.pth\", map_location=device))\n",
        "    vqvae_model.to(device)\n",
        "\n",
        "    # Instantiate and train the diffusion model\n",
        "    diffusion_model = LatentDiffusion(latent_dim=64, hidden_dim=128)\n",
        "    train_diffusion(diffusion_model, vqvae_model, dataloader, epochs=20, lr=1e-3, device=device)\n",
        "\n",
        "    # Save the trained diffusion model for later sampling\n",
        "    torch.save(diffusion_model.state_dict(), \"diffusion_model.pth\")"
      ],
      "metadata": {
        "id": "N0T87j4UFJhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import open3d as o3d\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def sample_latent(diffusion_model, latent_dim=64, noise_level=0.1, device='cpu'):\n",
        "    print(\"\\n Sampling latent code using diffusion model...\")\n",
        "    # Start with random noise\n",
        "    z_noisy = torch.randn(1, latent_dim).to(device) * noise_level\n",
        "    print(\"\\n Random noise vector shape:\", z_noisy.shape)\n",
        "    # Use the diffusion model to predict a clean latent code\n",
        "    with torch.no_grad():\n",
        "        z_sampled = diffusion_model(z_noisy)\n",
        "    print(\"\\n Sampled latent vector shape:\", z_sampled.shape)\n",
        "    return z_sampled\n",
        "\n",
        "def render_point_cloud(points_np):\n",
        "    print(\"\\n Rendering point cloud...\")\n",
        "    # Extract x, y, z coordinates from the numpy array\n",
        "    x, y, z = points_np[:, 0], points_np[:, 1], points_np[:, 2]\n",
        "\n",
        "    # Compute the distance from the origin for each point to use for color mapping\n",
        "    distances = np.sqrt(x**2 + y**2 + z**2)\n",
        "\n",
        "    # Create the Plotly figure with improved styling\n",
        "    fig = go.Figure(data=[go.Scatter3d(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        z=z,\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=3,            # Smaller marker size for denser clouds\n",
        "            color=distances,   # Color by distance from the origin\n",
        "            colorscale='Viridis',  # Use the Viridis colormap\n",
        "            opacity=0.8,\n",
        "            colorbar=dict(title=\"Distance\")\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    # Update layout with titles and custom background\n",
        "    fig.update_layout(\n",
        "        title=\"3D Point Cloud Visualization\",\n",
        "        scene=dict(\n",
        "            xaxis_title='X Axis',\n",
        "            yaxis_title='Y Axis',\n",
        "            zaxis_title='Z Axis',\n",
        "            bgcolor='rgb(240,240,240)'\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=30)\n",
        "    )\n",
        "\n",
        "    # Display the interactive Plotly figure\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Sampling and Rendering Demo\n",
        "# -----------------------------\n",
        "# Set device based on availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Instantiate and load your models (ensure VQVAE and LatentDiffusion are defined)\n",
        "print(\"\\n Loading VQ-VAE model...\")\n",
        "vqvae_model = VQVAE(input_dim=3, hidden_dim=128, latent_dim=64, num_embeddings=512, num_points=1024)\n",
        "vqvae_model.load_state_dict(torch.load(\"vqvae_model.pth\", map_location=device))\n",
        "vqvae_model.to(device)\n",
        "print(\"\\n VQ-VAE model loaded.\")\n",
        "\n",
        "print(\"\\n Loading Latent Diffusion model...\")\n",
        "diffusion_model = LatentDiffusion(latent_dim=64, hidden_dim=128)\n",
        "diffusion_model.load_state_dict(torch.load(\"diffusion_model.pth\", map_location=device))\n",
        "diffusion_model.to(device)\n",
        "print(\"Latent Diffusion model loaded.\\n\")\n",
        "\n",
        "# Sample a latent code using the diffusion model\n",
        "z_sample = sample_latent(diffusion_model, latent_dim=64, noise_level=0.1, device=device)\n",
        "\n",
        "# Decode the latent code to obtain a 3D point cloud\n",
        "print(\"\\n Decoding latent code into 3D point cloud...\")\n",
        "with torch.no_grad():\n",
        "    point_cloud = vqvae_model.decoder(z_sample)\n",
        "print(\"Decoded point cloud tensor shape:\", point_cloud.shape)\n",
        "\n",
        "# Convert the decoded point cloud tensor to numpy\n",
        "point_cloud_np = point_cloud.squeeze(0).cpu().numpy()  # shape: [num_points, 3]\n",
        "print(\"\\n Final point cloud numpy array shape:\", point_cloud_np.shape)\n",
        "\n",
        "# Render the point cloud using the Plotly visualizer (works in Colab)\n",
        "render_point_cloud(point_cloud_np)\n",
        "\n",
        "print(\"\\n Process complete.\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "etDwb2LTIBJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}